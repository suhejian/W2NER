{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就拿`conll03`数据集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, config_data):\n",
    "        config = config_data\n",
    "\n",
    "        self.dataset = config[\"dataset\"]\n",
    "\n",
    "        self.dist_emb_size = config[\"dist_emb_size\"]\n",
    "        self.type_emb_size = config[\"type_emb_size\"]\n",
    "        self.lstm_hid_size = config[\"lstm_hid_size\"]\n",
    "        self.conv_hid_size = config[\"conv_hid_size\"]\n",
    "        self.bert_hid_size = config[\"bert_hid_size\"]\n",
    "        self.biaffine_size = config[\"biaffine_size\"]\n",
    "        self.ffnn_hid_size = config[\"ffnn_hid_size\"]\n",
    "\n",
    "        self.dilation = config[\"dilation\"]\n",
    "\n",
    "        self.emb_dropout = config[\"emb_dropout\"]\n",
    "        self.conv_dropout = config[\"conv_dropout\"]\n",
    "        self.out_dropout = config[\"out_dropout\"]\n",
    "\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.weight_decay = config[\"weight_decay\"]\n",
    "        self.clip_grad_norm = config[\"clip_grad_norm\"]\n",
    "        self.bert_name = config[\"bert_name\"]\n",
    "        self.bert_learning_rate = config[\"bert_learning_rate\"]\n",
    "        self.warm_factor = config[\"warm_factor\"]\n",
    "\n",
    "        self.use_bert_last_4_layers = config[\"use_bert_last_4_layers\"]\n",
    "\n",
    "        self.seed = config[\"seed\"]\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}\".format(self.__dict__.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import data_loader\n",
    "\n",
    "with open(\"./config/conll03.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "# with open(\"./data/{}/train.json\".format(config_data['dataset']), \"r\", encoding=\"utf-8\") as f:\n",
    "#     train_data = json.load(f)\n",
    "\n",
    "# with open(\"./data/{}/dev.json\".format(config_data['dataset']), \"r\", encoding=\"utf-8\") as f:\n",
    "#     train_data = json.load(f)\n",
    "\n",
    "# with open(\"./data/{}/test.json\".format(config_data['dataset']), \"r\", encoding=\"utf-8\") as f:\n",
    "#     test_data = json.load(f)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config_data['bert_name'], cache_dir=\"./cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(config_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 10:22:24 - INFO: dict_items([('dataset', 'conll03'), ('dist_emb_size', 20), ('type_emb_size', 20), ('lstm_hid_size', 768), ('conv_hid_size', 96), ('bert_hid_size', 1024), ('biaffine_size', 768), ('ffnn_hid_size', 128), ('dilation', [1, 2, 3]), ('emb_dropout', 0.5), ('conv_dropout', 0.5), ('out_dropout', 0.33), ('epochs', 10), ('batch_size', 12), ('learning_rate', 0.001), ('weight_decay', 0), ('clip_grad_norm', 1.0), ('bert_name', 'bert-large-cased'), ('bert_learning_rate', 1e-05), ('warm_factor', 0.1), ('use_bert_last_4_layers', True), ('seed', 123)])\n",
      "2022-05-12 10:22:24 - INFO: dict_items([('dataset', 'conll03'), ('dist_emb_size', 20), ('type_emb_size', 20), ('lstm_hid_size', 768), ('conv_hid_size', 96), ('bert_hid_size', 1024), ('biaffine_size', 768), ('ffnn_hid_size', 128), ('dilation', [1, 2, 3]), ('emb_dropout', 0.5), ('conv_dropout', 0.5), ('out_dropout', 0.33), ('epochs', 10), ('batch_size', 12), ('learning_rate', 0.001), ('weight_decay', 0), ('clip_grad_norm', 1.0), ('bert_name', 'bert-large-cased'), ('bert_learning_rate', 1e-05), ('warm_factor', 0.1), ('use_bert_last_4_layers', True), ('seed', 123)])\n",
      "2022-05-12 10:22:34 - INFO: \n",
      "+---------+-----------+----------+\n",
      "| conll03 | sentences | entities |\n",
      "+---------+-----------+----------+\n",
      "|  train  |   17291   |  29441   |\n",
      "|   dev   |    3453   |   5648   |\n",
      "|   test  |    3453   |   5648   |\n",
      "+---------+-----------+----------+\n",
      "2022-05-12 10:22:34 - INFO: \n",
      "+---------+-----------+----------+\n",
      "| conll03 | sentences | entities |\n",
      "+---------+-----------+----------+\n",
      "|  train  |   17291   |  29441   |\n",
      "|   dev   |    3453   |   5648   |\n",
      "|   test  |    3453   |   5648   |\n",
      "+---------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "logger = utils.get_logger(config.dataset)\n",
    "logger.info(config)\n",
    "config.logger = logger\n",
    "datasets = data_loader.load_data_bert(config)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader, dev_loader, test_loader = (\n",
    "    DataLoader(dataset=dataset,\n",
    "                batch_size=config.batch_size,\n",
    "                collate_fn=data_loader.collate_fn,\n",
    "                shuffle=i == 0,\n",
    "                num_workers=4,\n",
    "                drop_last=i == 0)\n",
    "    for i, dataset in enumerate(datasets)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于`dataset`中的`tensor`: `bert_inputs`, `grid_labels`, `grid_mask2d`, `pieces2word`, `dist_inputs`, `sent_length`, `entity_text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from model import Model\n",
    "\n",
    "model = Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "\n",
    "model_path = \"./model.pt\"\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "# 评估模式\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "pred_result = []\n",
    "label_result = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data_batch in enumerate(test_loader):\n",
    "        entity_text = data_batch[-1]\n",
    "        label_result.append(entity_text)\n",
    "        data_batch = [data.cuda() for data in data_batch[:-1]]\n",
    "        bert_inputs, grid_labels, grid_mask2d, pieces2word, dist_inputs, sent_length = data_batch\n",
    "\n",
    "        outputs = model(bert_inputs, grid_mask2d, dist_inputs, pieces2word, sent_length)\n",
    "        outputs = torch.argmax(outputs, -1)\n",
    "        predictions = utils.get_predictions(outputs.cpu().numpy(), entity_text, sent_length.cpu().numpy())\n",
    "        pred_result.append(predictions)\n",
    "        # label_result.append(grid_labels)\n",
    "        # pred_result.append(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "for i in range(len(pred_result)):\n",
    "    labels.extend(label_result[i])\n",
    "    preds.extend(pred_result[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'2-#-5', '7-#-4'},\n",
       " {'0-1-#-4'},\n",
       " {'0-#-5', '2-3-4-#-5'},\n",
       " {'0-#-5', '15-#-5', '6-7-#-3'},\n",
       " {'1-#-5', '23-#-5'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[: 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sentence, entity_text):\n",
    "    \"\"\"\n",
    "    根据entity_text和原文得到真正的实体\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    for entity in entity_text:\n",
    "        ids = entity.split(\"-\")\n",
    "        ids = ids[: -2]\n",
    "        entity_ids = [int(x) for x in ids]\n",
    "        entity = [sentence[idx] for idx in entity_ids]\n",
    "        entities.append(\" \".join(entity))\n",
    "    return \",\".join(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isContinuous(entity):\n",
    "    \"\"\"\n",
    "    该实体是否连续\n",
    "    \"\"\"\n",
    "    continuous = True\n",
    "    ids = entity.split(\"-\")\n",
    "    ids = ids[: -2]\n",
    "    ids = [int(x) for x in ids]\n",
    "    start, end = ids[0], ids[-1]\n",
    "    if list(range(start, end + 1)) != ids:\n",
    "        continuous = False\n",
    "    return continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(test_data[0]['sentence'], labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写入预测文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./predictions/share_2013_pred.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(test_data)):\n",
    "        f.write(\"raw sentence: \\n\")\n",
    "        f.write(\" \".join(test_data[i]['sentence']) + \"\\n\")\n",
    "        f.write(\"true entities: \\n\")\n",
    "        f.write(get_entities(test_data[i]['sentence'], labels[i]) + \"\\n\")\n",
    "        f.write(\"predicted entities: \\n\")\n",
    "        f.write(get_entities(test_data[i]['sentence'], preds[i]) + \"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析预测文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先看整体表现和非连续实体表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_entities:  5333\n",
      "num_correct_entities:  4163\n",
      "num_preds:  5111\n",
      "num_dis_entities:  436\n",
      "num_correct_dis_entities:  219\n",
      "num_preds_dis:  344\n"
     ]
    }
   ],
   "source": [
    "# with open(\"./predictions/share_2013_pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "num_entities = 0    # 实体数量\n",
    "num_correct_entities = 0     # 正确实体数量\n",
    "num_preds = 0    # 预测实体数量\n",
    "\n",
    "num_dis_entities = 0    # 非连续实体数量\n",
    "num_correct_dis_entities = 0    # 正确非连续实体数量\n",
    "num_preds_dis = 0    # 预测非连续实体数量\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    true_entity = labels[i]\n",
    "    num_entities += len(true_entity)\n",
    "\n",
    "    pred_entity = preds[i]\n",
    "    num_preds += len(pred_entity)\n",
    "    \n",
    "    for entity in true_entity:\n",
    "        if entity in pred_entity:\n",
    "            num_correct_entities += 1\n",
    "            if isContinuous(entity) == False:\n",
    "                num_correct_dis_entities += 1\n",
    "        if isContinuous(entity) == False:\n",
    "            num_dis_entities += 1\n",
    "    \n",
    "    for entity in pred_entity:\n",
    "        if isContinuous(entity) == False:\n",
    "            num_preds_dis += 1\n",
    "\n",
    "\n",
    "print(\"num_entities: \", num_entities)\n",
    "print(\"num_correct_entities: \", num_correct_entities)\n",
    "print(\"num_preds: \", num_preds)\n",
    "print(\"num_dis_entities: \", num_dis_entities)\n",
    "print(\"num_correct_dis_entities: \", num_correct_dis_entities)\n",
    "print(\"num_preds_dis: \", num_preds_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import get_relation_matrix\n",
    "from statistics import count_relation\n",
    "\n",
    "count_relation(get_relation_matrix(test_data[12], labels[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis_utils\n",
    "\n",
    "# 得到含有非连续实体的句子的索引\n",
    "dis_indexes = []\n",
    "\n",
    "# 得到不含有实体的句子的索引\n",
    "no_entity_indexes = []\n",
    "\n",
    "# 得到只含有连续实体的句子的索引\n",
    "entity_indexes = []\n",
    "\n",
    "for i, sample in enumerate(test_data):\n",
    "    sentence = sample[\"sentence\"]\n",
    "    ner = sample[\"ner\"]\n",
    "    dis_ner = []\n",
    "    if not ner:\n",
    "        no_entity_indexes.append(i)\n",
    "    else:\n",
    "        for item in ner:\n",
    "            indexes = item['index']\n",
    "\n",
    "            if not dis_utils.isContinuous(indexes):\n",
    "                dis_ner.append(item)\n",
    "        if not dis_ner:\n",
    "            entity_indexes.append(i)\n",
    "        else:\n",
    "            dis_indexes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./predictions/share_2013_pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_entity_error_lines = []\n",
    "dis_error_lines = []\n",
    "entity_error_lines = []\n",
    "for i in range(len(lines)):\n",
    "    if i % 7 == 0:\n",
    "        true_entity = lines[i + 3].strip()\n",
    "        pred_entity = lines[i + 5].strip()\n",
    "        if sorted(true_entity.split(\",\")) != sorted(pred_entity.split(\",\")):\n",
    "            index = int(i / 7)\n",
    "            if index in no_entity_indexes:\n",
    "                no_entity_error_lines.extend(lines[i: i + 7])\n",
    "            elif index in dis_indexes:\n",
    "                dis_error_lines.extend(lines[i: i + 7])\n",
    "            elif index in entity_indexes:\n",
    "                entity_error_lines.extend(lines[i: i + 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./predictions/share_2013_error/no_entity_error.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in no_entity_error_lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./predictions/share_2013_error/dis_entity_error.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in dis_error_lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./predictions/share_2013_error/entity_error.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in entity_error_lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实体为空的句子数：5910\n",
      "占整个测试集的比例：0.656010656010656\n",
      "预测为空实体的句子数：6002\n",
      "占整个测试集的比例：0.6662226662226662\n"
     ]
    }
   ],
   "source": [
    "num_predict_no_entity = 0   # 预测为空实体的句子数\n",
    "num_no_entity = 0   # 实体为空的句子数\n",
    "num_sentences = 0   # 总句子数\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    if i % 7 == 0:\n",
    "        num_sentences += 1\n",
    "        true_entity = lines[i + 3].strip()\n",
    "        pred_entity = lines[i + 5].strip()\n",
    "        if true_entity == \"\":\n",
    "            num_no_entity += 1\n",
    "        if pred_entity == \"\":\n",
    "            num_predict_no_entity += 1\n",
    "print(f\"实体为空的句子数：{num_no_entity}\")\n",
    "print(\"占整个测试集的比例：{}\".format(num_no_entity / num_sentences))\n",
    "print(f\"预测为空实体的句子数：{num_predict_no_entity}\")\n",
    "print(\"占整个测试集的比例：{}\".format(num_predict_no_entity / num_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实体数目：11244\n",
      "预测实体数目：11114\n"
     ]
    }
   ],
   "source": [
    "num_entities = 0   # 实体数目\n",
    "num_pred_entities = 0   # 预测实体数目\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    if i % 7 == 0:\n",
    "        true_entity = lines[i + 3].strip()\n",
    "        num_entities += len(true_entity.split(\",\"))\n",
    "\n",
    "        pred_entity = lines[i + 5].strip()\n",
    "        num_pred_entities += len(pred_entity.split(\",\"))\n",
    "print(f\"实体数目：{num_entities}\")\n",
    "print(\"预测实体数目：{}\".format(num_pred_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计关系数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有3719483个关系\n",
      "一共有3710320个关系为0\n",
      "一共有3834个关系为1\n",
      "一共有5329个关系为2\n",
      "一共预测3719483个关系\n",
      "一共预测3710420个关系为0\n",
      "一共预测3964个关系为1\n",
      "一共预测5099个关系为2\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "num0 = 0\n",
    "num1 = 0\n",
    "num2 = 0\n",
    "pre_num = 0\n",
    "pre_num0 = 0\n",
    "pre_num1 = 0\n",
    "pre_num2 = 0\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    sample = test_data[i]\n",
    "    label = labels[i]\n",
    "    pred = preds[i]\n",
    "\n",
    "    truth_matrix = get_relation_matrix(sample, label)\n",
    "    pred_matrix = get_relation_matrix(sample, pred)\n",
    "\n",
    "    a, b, c, d = count_relation(truth_matrix)\n",
    "    num0 += a\n",
    "    num1 += b\n",
    "    num2 += c\n",
    "    num += d\n",
    "\n",
    "    a1, b1, c1, d1 = count_relation(pred_matrix)\n",
    "    pre_num0 += a1\n",
    "    pre_num1 += b1\n",
    "    pre_num2 += c1\n",
    "    pre_num += d1\n",
    "\n",
    "\n",
    "print(f'一共有{num}个关系')\n",
    "print(f'一共有{num0}个关系为0')\n",
    "print(f'一共有{num1}个关系为1')\n",
    "print(f'一共有{num2}个关系为2')\n",
    "print(f'一共预测{pre_num}个关系')\n",
    "print(f'一共预测{pre_num0}个关系为0')\n",
    "print(f'一共预测{pre_num1}个关系为1')\n",
    "print(f'一共预测{pre_num2}个关系为2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e15fdaeffecca357aa20502a01e04b649bba467ef26e9437ee6a11c3e9eebbb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('w2ner')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
